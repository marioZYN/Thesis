#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\options openright
\use_default_options false
\master ../thesis.lyx
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Indice
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swiss
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
In this chapter, we talk about Count-ception architecture which is based
 on Inception modules and fully convolutional network.
 Inception network is invented by Google and discussed in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/CohenLB17"
literal "false"

\end_inset

, and fully convolutional network is first analyzed in 
\begin_inset CommandInset citation
LatexCommand cite
key "43022"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Inception Network
\end_layout

\begin_layout Standard
The Inception network is an important milestone of CNN classifiers.
 Before Inception network, the most straight forward way of improving the
 performance of deep neural networks is by increasing their size 
\begin_inset CommandInset citation
LatexCommand cite
key "43022"
literal "false"

\end_inset

.
 This includes both increasing the depth - the number of levels - of the
 network and its width: the number of units at each level.
 This is as an easy and safe way of training higher quality models, only
 when we have a large amount of labeled training data.
 Also bigger size typically means a large number of parameters, which makes
 the enlarged network more prone to overfitting.
 The Inception network is carefully designed in terms of speed and accuracy
 while keeping the computational budget constant.
 The network exploits a good local network topology (network within a network)
 and then stack these modules on top of each other.
 The performance is verified by GoogleLeNet, a 22-layer deep network which
 won ILSVRC14 competition.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/element.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
GoogleLeNet
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The main difference between Inception module and normal CNN convolution
 layer is that Inception uses various sizes of filters in each convolution
 layer while CNN uses only one.
 Convolution filter sizes define how much local information we would like
 to collect.
 When we increase its size, we tend to collect more spatial information
 and can create more sparse features.
 By using different filter sizes, we can learn both the sparse and non-sparse
 features in each layer, thus increasing the width of the network.
 The outputs of each filter are stacked together to form the input of the
 next stage.
 As we can see in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inception-module"
plural "false"
caps "false"
noprefix "false"

\end_inset

, there are three shapes of filter: 1x1, 3x3 and 5x5.
 Choosing these specific filter sizes is not mandatory and we could use
 other sizes we like.
 Note that in order to stack output tensors, we need to have outputs with
 same dimension from each filter and we can achieve it by using padding.
 The 1x1 filter is used for dimension reduction by decreasing output channels.
 
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inception-module,-naive"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see the naive implementation of an Inception module.
 This implementation, however, has one big issue, the number of channels
 in the output tensor can explode.
 Since pooling operation does not change the channel number and we stack
 all intermediate tensors along the depth, the final output tensor will
 have much more channels than the input tensor.
 This problem becomes even more pronounced when we chain more Inception
 modules.
 The second implementation structure solves channel exploding problem, as
 we can see in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inception-module-with"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Whenever the computational requirements increase too much, we can apply
 a 1x1 convolution to reduce the dimension.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/inception_model_native.png
	lyxscale 30
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Inception-module,-naive"

\end_inset

Inception module, naive version
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/inception_model_real.png
	lyxscale 30
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Inception-module-with"

\end_inset

Inception module with dimension reductions
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Inception-module"

\end_inset

Inception module
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Inception module is the fundamental element in Inception network.
 By concatenating various sizes of filters in each layer and using 1x1 filters
 to reduce dimensions, the network can grow wider and deeper.
 Since Inception module is invented, several improvements are made over
 the years.
 However in this thesis, we only use the idea of stacking various sizes
 of filters and ignore other tricks.
 So the details of the improvements for Inception networks are not discussed
 here, only a summary is provided.
 
\end_layout

\begin_layout Itemize
Inception v1 concatenates Inception modules to make the network wider and
 deeper.
\begin_inset CommandInset citation
LatexCommand cite
key "43022"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
Inception v2 uses two 3 x 3 filters to replace 5 x 5 filters, decomposes
 n x n filers into 1 x n and n x 1 filters in order to increase computation
 speed.
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/SzegedyVISW15"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
Inception v3 introduces RMSprop optimization algorithm, factorized 7 x 7
 filters and batch normalization.
 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/SzegedyVISW15"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
Inception v4 exploits the idea of ResNet.
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/SzegedyIV16"
literal "false"

\end_inset


\end_layout

\begin_layout Section
Fully Convolutional Network
\end_layout

\begin_layout Standard
FCN is short for fully convolutional neural net which is a convolutional
 network without fully connected layers.
 The whole network is built by convolution and pooling layers only.
 Normal CNN architectures can be seen as a pipeline structure: first using
 several convolution and pooling layers to extract features from images,
 then using fully connected layers to exploit these features for classification.
 This structure has one disadvantage, once we set up the architecture we
 can not change the input image size anymore, otherwise we can not forward
 the tensor into fully connected layers.
 So it is very common to see that in many CNN architectures, the first step
 is to crop or warp the input image into a certain size.
 In many cases, the damage for this resizing operation is underestimated.
 SPP-net uses spatial pyramid pooling to create fixed length feature vector,
 while fully convolutional network discards fully connected layers to accept
 different sized input images.
\end_layout

\begin_layout Standard
In fact, passing tensors through fully connected layers can be seen as a
 convolution operation.
 We can convert any fully connected layers into convolution, with one-to-one
 map on the weights.
 Let's see an example, suppose we have a CNN doing three class classification
 with a 128-neuron fully connected layer and we get a tensor with size 2
 x 2 x 256 after several convolution and pooling operations.
 If we want to pass this tensor through fully connected layers, we need
 to first stretch it into a long vector of size 1024.
 If we ignore the bias, the weight matrices in fully connected layers are
 1024 x 128 and 128 x 3.
 We can convert fully connected layers into convolution layers using the
 following steps:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/CNN.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Normal CNN architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/FCNN.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Fully Convolutional Network
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Do not stretch the 2 x 2 x 256 tensor, instead keep the dimension.
\end_layout

\begin_layout Enumerate
Build 128 filters, each filter is a tensor of 2 x 2 x 256.
 Passing 1024 length vector through fully connected layers can be seen as
 doing convolution with no padding and stride one.
\end_layout

\begin_layout Enumerate
Build 3 filters, each filter is 1 x 1 x 128.
 
\end_layout

\begin_layout Standard
If we do the math, there is a one-to-one map between weights in fully connected
 layers and weights in convolution filters, as we can see in the figure
 above.
 In general, converting any fully connected layers into convolution operations
 has the following rules:
\end_layout

\begin_layout Itemize
Passing vector through fully connected layers is equivalent to doing convolution
 with no padding and stride one.
\end_layout

\begin_layout Itemize
The filter size is equal to the size of input tensor, and number of filters
 is equal to the number of neurons in the next fully connected layer.
\end_layout

\begin_layout Standard
Converting fully connected layers into convolution layers has several benefits.
 First, we do not need to reshape the image when we have different image
 size.
 As long as the input image size is no smaller than the filter, we can directly
 forward it through the network.
 Second, we do not get a single vector at the output, instead we get a tensor.
 This means if we are doing image classification and we feed the network
 with an image having larger size, we will not get a single probability
 vector but a heat map.
 Thus we can exploit the heat map information for further processing.
 Third, modern deep learning frameworks like tesorflow and pytorch have
 optimization for convolution operations, so by doing computation in a fully
 convolutional manner, we can get the result faster than doing computation
 batch-wise.
\end_layout

\begin_layout Standard
Although SPP-net discards the non-convolutional portion of classification
 nets to make a feature extractor, it can not be learned end-to-end.
 Alternatively, fully convolutional network converts fully connected layers
 to convolution operation which makes it possible to train the whole network
 all at once.
 
\end_layout

\begin_layout Standard
If we analyze the performance of FCN, we will see that doing stochastic
 training under FCN is equivalent to doing batch-wise training under normal
 CNN 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/pami/ShelhamerLD17"
literal "false"

\end_inset

.
 A real-valued loss function composed with an FCN defines a task.
 If the loss function is a sum over the spatial dimensions of the final
 layer, 
\begin_inset Formula $\ell(x;\theta)=\sum_{ij}\ell^{'}(x_{ij};\theta)$
\end_inset

, its gradient will be a sum over the gradients of each of its spatial component
s.
 Thus stochastic gradient descent on 
\begin_inset Formula $\ell$
\end_inset

 computed on whole images will be the same as stochastic gradient descent
 on 
\begin_inset Formula $\ell^{'}$
\end_inset

, taking all of the final layer receptive fields as a mini-batch.
 When these receptive fields overlap significantly, both feedforward computation
 and back propagation are much more efficient when computed layer-by-layer
 over an entire image instead of independently patch-by-patch.
 
\end_layout

\begin_layout Section
Count-ception Architecture
\end_layout

\begin_layout Standard
Count-ception network is introduced in paper 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/CohenLB17"
literal "false"

\end_inset

.
 The author uses Inception modules to build a network targeting at counting
 objects in the image.
 The whole network is a fully convolutional neural net and no pooling layer
 is used.
 The author didn't use pooling layers in order to not lose pixel information
 and make calculating receptive field easier.
 The network is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Count-ception-Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which is used for regression, each 32 x 32 region produces a 1 x 1 x 1
 tensor indicating the number of objects contained in this region.
 After each convolution, batch normalization and leaky ReLU activation are
 used in order to speed up convergence.
 The 3 x 3 convolutions are padded so they do not reduce the size and there
 are only two points in the network where the size is reduced.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/Count-ception.png
	lyxscale 30
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Count-ception-Architecture"

\end_inset

Count-ception Architecture 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Instead of taking the entire image as input and produces a single prediction
 for the number of objects, Count-ception is a smaller network that is run
 over the image to produce an intermediate count map
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/CohenLB17"
literal "false"

\end_inset

.
 This smaller network is trained to count the number of objects in its receptive
 field.
 Moreover, the input image I is processed in a fully convolutional way to
 produce a matrix F(I) that represents the counts of objects for a specific
 receptive field r x r of a sub-network that performs the counting.
 
\end_layout

\begin_layout Standard
A prediction map is generated using an input image and Count-ception architectur
e, and we need to define a loss function in order to use gradient descent
 to update filter weights, making our network learn to count.
 The target we are learning is generated from the dot labeled image and
 by using convolution, we create a target map from this dot input.
 This convolution filter size is the same as the receptive field in Count-ceptio
n architecture.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Count-ception-pipeline"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the training pipeline is illustrated.
 Pixel-wised L1 loss is calculated between prediction map and target map.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min||F(I)-T||_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/Cout-ception pipeline.png
	lyxscale 60
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Count-ception-pipeline"

\end_inset

Count-ception pipeline
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The whole procedure for Count-ception is the following:
\end_layout

\begin_layout Enumerate
Pad the input image in order to deal with objects near the boarder.
\end_layout

\begin_layout Enumerate
Calculate the prediction map using Count-ception architecture.
\end_layout

\begin_layout Enumerate
Calculate the target map using convolution.
\end_layout

\begin_layout Enumerate
Calculate the loss between prediction map and target map.
 The loss function is L1 loss.
\end_layout

\begin_layout Enumerate
Use gradient descent to update the weights.
\end_layout

\begin_layout Enumerate
Sum up all the pixel values in prediction map in order to get object count
 prediction.
 
\end_layout

\begin_layout Standard
After we have trained the Count-ception network, we can calculate the prediction
 map for each input image.
 In order to get the number of objects in the image, we sum up all the pixel
 values in the prediction map.
 Note that due to convolution operation, each pixel in the input image is
 counted redundantly.
 The network is designed intentionally to count each cell multiple times
 in order to average over possible errors.
 With a stride of 1, each target is counted once for each pixel in its receptive
 field.
 We can adjust the object counts using the following formula.
 F(I) stands for the prediction map, and r is the receptive field size.
 In the example above, the receptive field of one pixel in the output is
 32 x d32, so r equals 32.
 Each pixel in the input image is counted 32x32 times.
 
\begin_inset Formula 
\[
counts=\frac{\sum_{x,y}F(I)}{r^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
With the above approach, we sacrifice the ability to localize each cell
 exactly with x, y coordinates, however for many applications accurate counting
 is more important that exact localization.
 Another issue with this approach is that a correct overall count may not
 come from correctly identifying cells and could be the network adapting
 to the average prediction for each regression 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/CohenLB17"
literal "false"

\end_inset

.
 One common example is if the training data contains many images without
 cells, the network may predict 0 in order to minimize the loss.
 A solution to this is to first train on a more balanced dataset and then
 take the well performing networks fine-tuning it on more sparse datasets.
 In our modified version of Count-ception network, we invent a weight balanced
 layer to deal with this issue.
 The details are provided in chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:third-chapter"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\end_body
\end_document
