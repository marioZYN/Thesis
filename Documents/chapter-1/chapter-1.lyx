#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\options openright
\use_default_options false
\master ../thesis.lyx
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Indice
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swiss
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Image classification, object detection and localization are one of the major
 challenges in computer vision.
 In this chapter we briefly introduce these problems and the state-of-art
 algorithms.
 Then we analyze how we may use these algorithms to solve our sea lion counting
 problem.
 
\end_layout

\begin_layout Section
Image classification
\end_layout

\begin_layout Standard
Image classification is the task of giving an input image and outputting
 the corresponding label.
 Before convolution neural network in image classification, people use handcraft
ed features from images and exploit these features for classifying images.
 It is a challenge, even for experts to design a feature extraction algorithm
 suitable for various vision recognization tasks.
 Convolutional neural network(CNN) is a special kind of deep learning architectu
re used in computer vision, which is composed of convolution and pooling
 layers.
 The convolution layer makes use of a set of learnable filters.
 Each filter learns how to extract features and patterns present in the
 image.
 The filter is convolved across the width and height of the input image,
 and a dot product operation is computed to give an activation map.
 Different filters which detect different features are convolved with the
 input image and the activation maps are stacked together to form the input
 for the next layer.
 By stacking more activation maps, we can get more abstract features.
 However, as the architecture becomes deeper, we may consume too much memory
 and in order to solve this problem, pooling layers are used to reduce the
 dimension of the activation maps.
 There are two types of pooling layers: max pooling and average pooling.
 As the name states, max pooling keeps the maximum value within the filter
 and discards all the rest, while average pooling keeps the average value.
 By discarding some values in each filter, we reduce the dimension of the
 activation maps and thus reduce the number of parameters we need to learn
 and this makes deep CNN architecture possible.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/CNN2.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Convolutional neural network
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/CNN.jpeg
	lyxscale 30
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:CNN-architecture-for"

\end_inset

CNN architecture for image classification
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
After each convolution operation, an activation function is added to decide
 whether a certain neuron fires or not.
 There are different kinds of activation functions having different characterist
ics as illustrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Activation-functions"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Sigmoid functions squashes the output into a value between zero and one
 and it was the most popular activation function back in days since it has
 nice interpretation as a saturating "firing rate" of a neuron.
 However sigmoid activation function has three major problems:
\end_layout

\begin_layout Enumerate
Saturated neurons "kill" the gradients.
\end_layout

\begin_layout Enumerate
Sigmoid outputs are not zero-centered which hurts gradient descent process.
\end_layout

\begin_layout Enumerate
Exponential function is a bit compute expensive.
\end_layout

\begin_layout Standard
ReLU(Rectified Linear Unit) activation function is used to avoid the drawbacks
 of sigmoid functions.
 ReLU activation function does not have saturation problem and while the
 largest gradient value for sigmoid function is 1/4, the gradient for ReLU
 function is either 1 or 0.
 Theoretically ReLU activation function has larger convergence rate than
 sigmoid function.
 The problem for ReLU function is that when we have a negative input value,
 the gradient is zero.
 It seems to behave like our neurons which can fire or not, but in reality
 this can create dead ReLU nodes.
 Since the value and gradient are all zero when the input value is negative,
 it can happen that some neurons can never fire again.
 This is called the "dead neuron phenomenon".
 In order to solve this problem, leaky ReLU is used.
 Leaky ReLU does not have zero gradient at the negative part of the axis,
 but a small positive value, thus when necessary the output value can grow
 back to non zero, avoiding dead neuron problem.
 Nowadays leaky ReLU is the most commonly used activation function in deep
 learning architectures.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/activation functions.png
	lyxscale 30
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Activation-functions"

\end_inset

Activation functions
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The general idea for CNN architecture is to stack several convolution and
 pooling layers to extract features from images, then it uses fully connected
 layers to exploit these features for classification.
 By using CNN architecture we don't need to design feature extraction algorithm,
 instead we can exploit gradient descent, letting convolution filters learn
 the weights from our training dataset.
 So convolution and pooling layers in CNN actually build up an automatic
 way for extracting features.
 
\end_layout

\begin_layout Standard
We want CNN architecture to generate classification results, and one way
 to do this is to output probability scores for each class.
 Suppose we have to classify each image among 
\begin_inset Formula $N$
\end_inset

 possible classes, we can make our CNN architecture generate 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $N$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 values, each value representing the probability of being a certain class.
 Since this is a probability distribution, the summation of these N values
 is one.
 In CNN architecture, softmax layer is inserted at last in order to squeeze
 the output between zero and one.
 Softmax function amplifies probability of largest 
\begin_inset Formula $x_{i}$
\end_inset

 but still assigns some probability to smaller 
\begin_inset Formula $x_{i}.$
\end_inset

 It is defined in Formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:softmax"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Formula 
\begin{equation}
P(y_{i}|x_{i};W)=\frac{e^{f_{y_{i}}}}{\mathop{{\displaystyle \sum_{j}e^{f_{j}}}}}\label{eq:softmax}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Object detection and localization
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/object detection.png
	lyxscale 30
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Object-detection-and"

\end_inset

Object detection and localization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Object detection and localization is a more difficult task than image classifica
tion, because you need to first find possible object locations and then
 perform object classification.
 Given an input image possibly containing multiple objects, we need to generate
 a bounding box around each object and classify the object type, as illustrated
 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Object-detection-and"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The general idea is to output the class label as well as the coordinates
 of the four corners of the bounding box.
 Outputting the class label is a classification problem and generating bounding
 box coordinates can be seen as a regression problem.
 In fact, each bounding box can be represented as a four value tuple: (
\begin_inset Formula $x,y,w,h)$
\end_inset

 which stands for coordinates of the center point, width and height of the
 bounding box.
 We combine the classification loss and regression loss as the final loss
 for our architecture.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/classification and regression.png
	lyxscale 60
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Classification and localization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Object detection and localization is a two-step problem, first we need to
 find which parts of the image may contain an object, second we need to
 classify each part.
 We call the region in the image where may exist an object "region of interests"
(ROI).
 One straight forward way to find ROI is to use a sliding window to generate
 all possible ROI.
 Since we don't know neither the location nor the size of each object, we
 need to test many positions and scales which is time consuming and not
 feasible.
 There exists various kinds of object detection and localization algorithms
 differing in network structure and ROI proposing techniques.
 Just like image classification algorithms, the performance of detection
 and localization boosted since deep learning architectures are used in
 this field, as we can see it in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Object-detection-renaissance"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In the following sections, we will introduce some sate-of-art object detection
 and localization algorithms and by studying them, we may get inspired of
 how to solve multi-object counting problem.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/object detection renaissance.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Object-detection-renaissance"

\end_inset

Object detection renaissance
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
R-CNN
\end_layout

\begin_layout Standard
R-CNN is short for Regional Convolutional Neural Network.
 The purpose of this algorithm is to generate a bounding box around each
 possible object and a corresponding label.
 Given an input image, we first generate ROI from the image and then use
 CNN to classify each region.
 
\end_layout

\begin_layout Standard
The pipeline for this algorithm is the following:
\end_layout

\begin_layout Enumerate
Build a CNN model and train it from scratch or download a pre-trained image
 classification model.
\end_layout

\begin_layout Enumerate
Fine-tune model for detection.
 Discard the final fully connected layer and modify it according to domain
 dependent problems.
 
\end_layout

\begin_layout Enumerate
Extract region proposals for all images by using external algorithms like
 selective search, and for each region, wrap it to CNN size and use CNN
 to classify its type.
 It can be a certain object or background.
 
\end_layout

\begin_layout Enumerate
Train one binary SVM per class to classify region features in order to check
 if object exists in this region.
\end_layout

\begin_layout Enumerate
For each class, train a linear regression model to map from extracted features
 of CNN to offsets of ground truth boxes in order to make up for the wrong
 positions of ROI.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/R-CNN.png
	lyxscale 50
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
R-CNN architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In R-CNN algorithm, the region proposal method is independent from the algorithm
 itself, and we can use whatever region proposal algorithm we like.
 In the paper, the author suggests to use "selective search".
 Selective search is used to separate image into different sized areas which
 may contain objects.
 The general idea is to first use edge detection methods for creating fine
 grilled chunks and then greedily merge similar ones to create ROI.
 It is a greedy algorithm, starting from bottom-up segmentation and merging
 regions at different scales.
 This method can find "blob-like" regions containing similar pixel values.
 For each image, there are around 
\begin_inset Formula $2k$
\end_inset

 regions of proposals.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/selective search.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Selective search
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
R-CNN was the start-of-art algorithm for object detection since 2010.
 The ad hoc training procedures for R-CNN is the following:
\end_layout

\begin_layout Itemize
Fine-tune network with softmax classifier
\end_layout

\begin_layout Itemize
Train post-hoc linear SVMs
\end_layout

\begin_layout Itemize
Train post-hoc bounding-box regressors
\end_layout

\begin_layout Standard
There are three major disadvantages for this algorithm, first of all it
 is slow at test time because it needs to run both the selective search
 and full forward path for each region proposal.
 With VGG16 as the base architecture, it takes around 47s per image of size
 224 x 224.
 Second, we spend large amount of time training SVMs and regressors, but
 they can only be used for this specific problem.
 For different problems we need to train SVMs and regressors again from
 scratch.
 Third, the whole architecture is not end-to-end and this complex multistage
 training pipeline is difficult to implement.
\end_layout

\begin_layout Subsection
SPP-net
\end_layout

\begin_layout Standard
SPP-net solves the problem of slow inference time in R-CNN architecture
 by sharing computation.
 Recall that in R-CNN, we first generate ROI and then classify each ROI
 using CNN.
 Since the original image may be large, we could generate a lot of ROI regions
 and each of them needs to be forwarded through a CNN, which is time consuming.
 Now SPP-net swaps the order by first forwarding the original image through
 a CNN architecture, and then generate ROI from the feature map we get out
 of convolution.
 Besides sharing computation, SPP-net also invents a new pooling operation
 called "Spatial Pyramid Pooling".
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/SPP-net.png
	lyxscale 60
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SPP-net"

\end_inset

SPP-net
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In CNN architecture, we have convolution and pooling layers to extract features
 from images, and then we use fully connected layers to exploit these features
 for classification.
 However there is a technical issue in the training and testing of the CNNs:
 most of CNNs require a fixed input size (e.g.
 224x224), which limits both the aspect ratio and the scale of the input
 image.
 When applied to images of arbitrary sizes, current CNN methods mostly fit
 the input image to the fixed size, either via cropping or via warping.
 However both methods have drawbacks: cropping can destroy whole image structure
 and wrapping can cause distortion.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/crop and warp.png
	lyxscale 80
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Crop-and-warp"

\end_inset

Crop and warp
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
So why do we need to have fixed sized input? In fact, convolution and pooling
 operations do not require fixed size input, only fully connected layers
 do.
 When we forward various sized images through convolution and pooling layers,
 what we get are only tensors with different shapes.
 On the other hand, fully connected layers require fixed sized input by
 their definition.
 SPP-net creates spatial pyramid pooling layers to deal with this issue.
 By adding SPP layer right before fully connected layer, we can create fixed
 sized feature vector from tensors in different shapes.
 SPP-layer is actually a sequence of max pooling operations combined together
 as described in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Spatial-pyramid-pooling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Each max pooling in SPP divides the input tensor evenly to a predefined
 number of areas.
 For example, the blue max pooling area always divides the whole tensor
 into 16 parts, no matter what shape the input tensor has.
 Each part will be max pooled to generate a single output value.
 Also note that 256-d is the number of channels of the input tensor, thus
 the final feature vector's length is calculated as: 
\begin_inset Formula $(16+4+1)\times256=5376$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/SPP.png
	lyxscale 60
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Spatial-pyramid-pooling"

\end_inset

Spatial pyramid pooling
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
SPP-net fixes one issue with R-CNN: it makes testing fast, but it still
 inherits the rest of R-CNN's problems:
\end_layout

\begin_layout Itemize
Ad hoc training objectives.
\end_layout

\begin_layout Itemize
Training is slow, taking a lot of disk space.
 
\end_layout

\begin_layout Standard
SPP-net also creates a new issue: we can not update parameters below SPP
 layer during training, since the SPP layer combines a sequence of max pooling
 operations of different filter sizes, the gradient can not flow through
 it because there is overlapping within these filters.
\end_layout

\begin_layout Subsection
Fast R-CNN
\end_layout

\begin_layout Standard
Fast R-CNN was invented as an improved version of R-CNN architecture.
 It exploits the idea of swapping the order of convolution and ROI proposing
 which makes it fast at test time, like SPP-net.
 It is also a network trained end-to-end, avoiding the complex multistage
 architecture in R-CNN.
 Fast R-CNN has higher mean average precision than R-CNN and SPP-net.
 The network is described in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast-R-CNN-architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/fast r-cnn.png
	lyxscale 30
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast-R-CNN-architecture"

\end_inset

Fast R-CNN architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After regions of proposals are generated, fast R-CNN architecture uses "ROI
 Pooling" to wrap ROI into a predefined size.
 ROI pooling is actually a single-level SPP layer.
 Fast R-CNN architecture can be trained end-to-end and we don't need to
 train separate modules like SVMs in R-CNN architecture.
 By using only a single-level SPP layer, gradients can flow across ROI pooling
 layer and training the whole network at once is possible.
 Fast R-CNN is a lot faster than R-CNN, and according to the author, the
 training phase is 8 times faster than R-CNN and it only takes 0.32 seconds
 to make an inference at test time.
 However this inference time does not include generating regions of proposals,
 and we still need to use other methods like selective search to find ROI.
 Since we have to use selective search as an independent method to find
 region of proposals, why don't we implement region proposing methods inside
 the network and let it learn how to generate proposals during training.
 Faster R-CNN was created based on this idea.
\end_layout

\begin_layout Subsection
Faster R-CNN
\end_layout

\begin_layout Standard
Summarizing all the algorithms above, we can realize that they all follow
 a similar pattern.
 First, we use some method to generate region proposals.
 Second, we use classification algorithms to classify each area and use
 regression to fine-tune bounding box positions.
 So far, faster R-CNN is the most powerful algorithm exploiting this two-step
 approach for object detection.
 Faster R-CNN inserts a "Region Proposal Network"(RPN) after the last convolutio
nal layer of fast R-CNN, thus avoids using selective search to generate
 ROI.
 RPN is trainable to produce region proposals directly and there is no need
 to use external region proposing methods.
 The rest of modules are the same as fast R-CNN.
 The whole structure is demonstrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Faster-R-CNN-architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and there are four major components in this network:
\end_layout

\begin_layout Enumerate
Convolution layers.
 Faster R-CNN exploits the basic idea of CNN, using convolution and pooling
 layers to extract feature maps from original image.
 These feature maps are later used in RPN and fully connected layers.
\end_layout

\begin_layout Enumerate
Region Proposal Network.
 For each pixel value in the feature maps, 9 anchors are created as candidates
 for region proposals and RPN uses softmax to classify each anchor as foreground
 or background (A foreground anchor contains an object).
 Then it uses bounding box regression to modify anchors for more precise
 proposals.
\end_layout

\begin_layout Enumerate
ROI pooling.
 This layer gathers feature maps and region proposals, making it into fixed
 length feature vector for fully connected layers.
\end_layout

\begin_layout Enumerate
Classification.
 Use the fixed length feature vector for classification and final bounding
 box regression.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/faster r-cnn.png
	lyxscale 50
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Faster-R-CNN-architecture"

\end_inset

Faster R-CNN architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
RPN is a small network for classifying object or non-object and it can regress
 bounding box locations as well.
 A sliding window is used on the convolutional feature maps and the position
 of the sliding window provides location information with reference to the
 image.
 As we can see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RPN"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the convolutional feature map has 256 channels and after another 3 x 3
 convolution to gather local information, we can generate a feature vector
 of length 256 for each pixel.
 This feature vector is used for both classification and bounding box regression.
 
\end_layout

\begin_layout Standard
Since we do not know in advance the size and shape of objects, for each
 pixel in the feature map we create 
\begin_inset Formula $k$
\end_inset

 anchor boxes around it as region proposal candidates.
 The reason for creating multiple anchor boxes is to match the proposal
 to the ground truth as close as possible.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Plane-anchor-boxes"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see that the green box is more precise.
 Anchors are represented by 4 values 
\begin_inset Formula $(c_{x},c_{y},width,height)$
\end_inset

 and they can act as foreground or background.
 So finally we get 
\begin_inset Formula $2k$
\end_inset

 scores and 
\begin_inset Formula $4k$
\end_inset

 coordinates as the output of RPN.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/plane anchor box.png
	lyxscale 50
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Plane-anchor-boxes"

\end_inset

Plane anchor boxes
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
How many anchors will we generate for each image? Suppose the original image
 is 800 x 600 x 3, and we use VGG16 as our feature extraction network.
 VGG16 downsamples an image to its 1/16, so if set 
\begin_inset Formula $k=9$
\end_inset

, the number of anchors is calculated as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
ceil(800/16)\times ceil(600/16)\times9=50\times3\times9=17100
\]

\end_inset


\end_layout

\begin_layout Standard
There are quite a lot anchors compared to selective search which generates
 around 
\begin_inset Formula $2k$
\end_inset

 proposals per image.
 Actually during training we sort anchor candidates by their classification
 scores and randomly select 256 positive and 256 negative ones as a training
 batch.
 Intersection of union (IoU) is used as a measure of the common area between
 an region proposal and ground truth bounding box.
 If IoU is larger than 0.7, we think it as a positive anchor and if it is
 smaller than 0.3, we view it as negative.
 Those anchors having 0.3 < IoU < 0.7 do not get involved in the training
 phase.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/RPN2.png
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:RPN"

\end_inset

RPN
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
RPN learns a regressor to map pixel values form the feature map to ROI bounding
 box coordinates.
 With RPN we do not need to generate ROI candidates as a preliminary step,
 and this makes inference a lot quicker.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:R-CNN-test-speed"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is a speed comparison among object detection algorithms we just introduced.
 All these algorithms exploit the idea of the two-step approach: First generate
 region proposals from original images.
 Second use classification techniques to classify each area and use regression
 to fine-tune bounding boxes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/RCNN test speed.png
	lyxscale 60
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:R-CNN-test-speed"

\end_inset

R-CNN test speed
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Counting objects by classification and detection
\end_layout

\begin_layout Standard
Can we use image classification and detection techniques for solving object
 counting problem? Recall that the problem we face is giving an input image,
 we need to estimate sections in the image and this requires both classification
 and object counting.
 Also note that the dataset we have does not include a bounding box around
 each object, but a colored dot at the center.
 Here is how we may use image classification and detection techniques to
 solve object counting problem:
\end_layout

\begin_layout Itemize
Classification: We can use a sliding window to generate patches from an
 image, and then use CNN architecture to classify each patch.
 Suppose we have 
\begin_inset Formula $N$
\end_inset

 kinds of objects in the image, and then including the background patch,
 we are now dealing with a 
\begin_inset Formula $N+1$
\end_inset

 classification problem.
 During inference, we first separate the image into patches and then classify
 each patch.
 We sum up all the patches to get the final result.
\end_layout

\begin_layout Itemize
Detection and localization: Since we do not have a bounding box around each
 object, in order to train an object detection network we need to first
 create a bounding box from the center dot.
 By approximating the size of each object, we could create the bounding
 box manually.
 Then we can do a faster R-CNN architecture to localize each object.
 During inference, each bounding box prediction indicates an object existence.
\end_layout

\begin_layout Standard
In principle both of the methods should work, but problems do exist for
 each of them.
 If we generate object counts by sliding window and image classification,
 we naturally assume that each patch maximumly contains a single object.
 In order to make count prediction precise, we need to carefully set the
 patch size so that most of the patches contain only one object or no object
 at all.
 As for object detection, it seems over-kill here because we don't need
 to predict object location while we only want to estimate object counts.
 In our sea lion counting problem, we tried to use sliding window and image
 classification as a start up.
 We first construct the training dataset by manually extracting patches
 with sea lions in the center.
 Then we train a CNN which learned how to classify each patch correctly.
 Finally we use the trained network to estimate sea lion numbers from testing
 images.
\end_layout

\end_body
\end_document
